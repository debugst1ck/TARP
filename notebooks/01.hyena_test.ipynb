{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34d174a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tarp.model.backbone.untrained.lstm import LstmEncoder\n",
    "from tarp.model.backbone.untrained.hyena import HyenaEncoder\n",
    "from tarp.model.backbone import Encoder\n",
    "from tarp.model.backbone.untrained.transformer import TransformerEncoder\n",
    "from tarp.model.finetuning.classification import ClassificationModel\n",
    "\n",
    "from tarp.services.datasets.classification.multilabel import MultiLabelClassificationDataset\n",
    "from tarp.services.tokenizers.pretrained.dnabert import Dnabert2Tokenizer\n",
    "from tarp.services.datasource.sequence import TabularSequenceSource, CombinationSource, FastaSliceSource\n",
    "\n",
    "\n",
    "from tarp.services.preprocessing.augmentation import (\n",
    "    CombinationTechnique,\n",
    "    RandomMutation,\n",
    "    InsertionDeletion,\n",
    "    ReverseComplement,\n",
    ")\n",
    "\n",
    "from tarp.services.datasets.metric.triplet import MultiLabelOfflineTripletDataset\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc4bb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_columns = (\n",
    "    pl.read_csv(Path(\"../temp/data/processed/labels.csv\")).to_series().to_list()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed61844b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MultiLabelClassificationDataset(\n",
    "    CombinationSource(\n",
    "        [\n",
    "            TabularSequenceSource(\n",
    "                source=Path(\"../temp/data/processed/card_amr.parquet\")\n",
    "            ),\n",
    "            FastaSliceSource(\n",
    "                directory=Path(\"../temp/data/external/sequences\"),\n",
    "                metadata=Path(\"../temp/data/processed/non_amr_genes_10000.parquet\"),\n",
    "                key_column=\"genomic_nucleotide_accession.version\",\n",
    "                start_column=\"start_position_on_the_genomic_accession\",\n",
    "                end_column=\"end_position_on_the_genomic_accession\",\n",
    "                orientation_column=\"orientation\",\n",
    "            ),\n",
    "        ]\n",
    "    ),\n",
    "    Dnabert2Tokenizer(),\n",
    "    sequence_column=\"sequence\",\n",
    "    label_columns=label_columns,\n",
    "    maximum_sequence_length=512,\n",
    "    augmentation=CombinationTechnique(\n",
    "        [\n",
    "            RandomMutation(),\n",
    "            InsertionDeletion(),\n",
    "            ReverseComplement(0.5),\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "\n",
    "metric_dataset = MultiLabelOfflineTripletDataset(\n",
    "    base_dataset=dataset, label_cache=Path(\"../temp/data/cache/labels_cache.parquet\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7403fa82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tarp.config import HyenaConfig, TransformerConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d6fd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = TransformerEncoder(\n",
    "    vocabulary_size=dataset.tokenizer.vocab_size,\n",
    "    embedding_dimension=TransformerConfig.embedding_dimension,\n",
    "    hidden_dimension=TransformerConfig.hidden_dimension,\n",
    "    padding_id=dataset.tokenizer.pad_token_id,\n",
    "    number_of_layers=TransformerConfig.number_of_layers,\n",
    "    number_of_heads=TransformerConfig.number_of_heads,\n",
    "    dropout=TransformerConfig.dropout,\n",
    ")\n",
    "\n",
    "classification_model = ClassificationModel(\n",
    "    encoder=encoder,\n",
    "    number_of_classes=len(label_columns),\n",
    ")\n",
    "\n",
    "# Filename of the checkpoint to load\n",
    "# Should be the latest checkpoint saved during training\n",
    "from pathlib import Path\n",
    "\n",
    "latest_checkpoint = max(Path(\"../temp/checkpoints/\").glob(\"*.pt\"), key=lambda p: p.stat().st_mtime)\n",
    "\n",
    "classification_model.load_state_dict(torch.load(latest_checkpoint.as_posix()))\n",
    "\n",
    "print(f\"Loaded model from checkpoint: {latest_checkpoint}\")\n",
    "\n",
    "# Get the encoder part of the model\n",
    "encoder = classification_model.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee7144e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the model to the dataset to get the embeddings\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Preallocate numpy array for all embeddings\n",
    "num_samples = len(dataset)\n",
    "batch_size = 32\n",
    "embedding_dim = encoder.encoding_size  # Output dimension of encoder.encode\n",
    "embeddings = np.empty((num_samples, embedding_dim), dtype=np.float32)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder.to(DEVICE)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "start_idx = 0\n",
    "encoder.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(dataloader):\n",
    "        input_ids = batch[\"sequence\"].to(DEVICE)\n",
    "        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "        batch_embeddings = encoder.encode(input_ids, attention_mask)\n",
    "        batch_size_actual = batch_embeddings.shape[0]\n",
    "        embeddings[start_idx:start_idx + batch_size_actual] = batch_embeddings.cpu().numpy()\n",
    "        start_idx += batch_size_actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6870838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shape of the embeddings\n",
    "print(\"Embeddings shape:\", embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537391ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "SEED = 69420\n",
    "indices = list(range(len(dataset)))\n",
    "train_indices, temp_indices = train_test_split(\n",
    "    indices, test_size=0.2, random_state=SEED\n",
    ")\n",
    "valid_indices, test_indices = train_test_split(\n",
    "    temp_indices, test_size=0.5, random_state=SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0f2021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train and test sets\n",
    "train_embeddings = embeddings[train_indices]\n",
    "test_embeddings = embeddings[test_indices]\n",
    "\n",
    "print(\"Train embeddings shape:\", train_embeddings.shape)\n",
    "print(\"Test embeddings shape:\", test_embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a254a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = np.array([\n",
    "    dataset[i]['labels'].numpy() for i in train_indices\n",
    "])\n",
    "test_labels = np.array([\n",
    "    dataset[i]['labels'].numpy() for i in test_indices\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a473598c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN search\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5, metric='cosine', weights='distance')\n",
    "\n",
    "knn.fit(train_embeddings, train_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09767a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(\n",
    "    train_labels,\n",
    "    knn.predict(train_embeddings),\n",
    "    zero_division=0,\n",
    "    target_names=label_columns\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d3d040",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(\n",
    "    test_labels,\n",
    "    knn.predict(test_embeddings),\n",
    "    zero_division=0,\n",
    "    target_names=label_columns\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bd9e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Every label which is not \"non-AMR\", extract the index in the\n",
    "amr_label_indices = [\n",
    "    idx for idx, label in enumerate(label_columns) if \"non_amr\" not in label\n",
    "]\n",
    "\n",
    "# Consolidate all the AMR class labels into a single \"AMR\" label and then Combine with non-AMR to get binary labels\n",
    "train_amr_binary = (train_labels[:, amr_label_indices].sum(axis=1) > 0).astype(int)\n",
    "test_amr_binary = (test_labels[:, amr_label_indices].sum(axis=1) > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e31dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_knn = KNeighborsClassifier(n_neighbors=5, metric='cosine', weights='distance')\n",
    "\n",
    "binary_knn.fit(train_embeddings, train_amr_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50f92a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    classification_report(\n",
    "        train_amr_binary,\n",
    "        binary_knn.predict(train_embeddings),\n",
    "        zero_division=0,\n",
    "        target_names=[\"non-AMR\", \"AMR\"],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d224475",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    classification_report(\n",
    "        test_amr_binary,\n",
    "        binary_knn.predict(test_embeddings),\n",
    "        zero_division=0,\n",
    "        target_names=[\"AMR\", \"non-AMR\"],\n",
    "        labels=[1, 0],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4c66a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import plotly.express as px\n",
    "\n",
    "# Import UMAP and t-SNE\n",
    "from umap import UMAP\n",
    "\n",
    "# Run UMAP on the embeddings\n",
    "umap = UMAP(n_components=2)\n",
    "train_embeddings_2d = umap.fit_transform(train_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7696bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for i in range(len(train_embeddings)):\n",
    "    active_labels = [label_columns[j] for j in np.where(train_labels[i] > 0)[0]]\n",
    "    for label in active_labels:\n",
    "        rows.append({\n",
    "            \"x\": train_embeddings_2d[i, 0],\n",
    "            \"y\": train_embeddings_2d[i, 1],\n",
    "            \"label\": label,\n",
    "            \"sample_index\": i,  # to identify duplicates\n",
    "        })\n",
    "\n",
    "df_vis = pl.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb456b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(\n",
    "    df_vis,\n",
    "    x=\"x\",\n",
    "    y=\"y\",\n",
    "    color=\"label\",\n",
    "    hover_data=[\"sample_index\"],\n",
    "    title=\"UMAP visualization of Hyena embeddings (multi-label)\",\n",
    "    labels={\"x\": \"UMAP dim 1\", \"y\": \"UMAP dim 2\", \"color\": \"Gene family\"},\n",
    "    opacity=0.7,\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    width=1024,\n",
    "    height=768,\n",
    "    legend_title=\"Label\",\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a05810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try t-SNE as well\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=SEED)\n",
    "\n",
    "train_embeddings_2d = tsne.fit_transform(train_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d9c108",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for i in range(len(train_embeddings)):\n",
    "    active_labels = [label_columns[j] for j in np.where(train_labels[i] > 0)[0]]\n",
    "    for label in active_labels:\n",
    "        rows.append({\n",
    "            \"x\": train_embeddings_2d[i, 0],\n",
    "            \"y\": train_embeddings_2d[i, 1],\n",
    "            \"label\": label,\n",
    "            \"sample_index\": i,  # to identify duplicates\n",
    "        })\n",
    "df_vis = pl.DataFrame(rows)\n",
    "fig = px.scatter(\n",
    "    df_vis,\n",
    "    x=\"x\",\n",
    "    y=\"y\",\n",
    "    color=\"label\",\n",
    "    hover_data=[\"sample_index\"],\n",
    "    title=\"t-SNE visualization of Hyena embeddings (multi-label)\",\n",
    "    labels={\"x\": \"t-SNE dim 1\", \"y\": \"t-SNE dim 2\", \"color\": \"Gene family\"},\n",
    "    opacity=0.7,\n",
    ")\n",
    "fig.update_layout(\n",
    "    width=1024,\n",
    "    height=768,\n",
    "    legend_title=\"Label\",\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091285ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test ClassificationModel for test data\n",
    "# Pytorch style classification with thresholds for multi-label classification\n",
    "\n",
    "logits = []\n",
    "labels = []\n",
    "\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "test_dataloader = DataLoader(Subset(dataset=dataset, indices=test_indices), batch_size=32, shuffle=False)\n",
    "\n",
    "classification_model.to(DEVICE)\n",
    "\n",
    "for batch in tqdm(test_dataloader):\n",
    "    input_ids = batch[\"sequence\"].to(DEVICE)\n",
    "    attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "    batch_logits = classification_model(input_ids, attention_mask)\n",
    "    logits.append(batch_logits.detach().cpu())\n",
    "    labels.append(batch[\"labels\"].detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded427e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(logits[0].shape)\n",
    "print(labels[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e850cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure both are 2D\n",
    "predictions = torch.cat(logits, dim=0)         # [N, num_labels]\n",
    "labels = torch.cat(labels, dim=0)             # [N, num_labels])\n",
    "preds = (torch.sigmoid(predictions) >= 0.5).int().cpu().numpy()\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(labels, preds, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ba6c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "thresholds = np.linspace(0.05, 0.95, 19)\n",
    "f1_scores = []\n",
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "\n",
    "for t in thresholds:\n",
    "    preds = (predictions >= t).int().cpu().numpy()\n",
    "    f1 = f1_score(labels, preds, average='micro')\n",
    "    f1_scores.append(f1)\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    accuracy_scores.append(accuracy)\n",
    "    precision = precision_score(labels, preds, average='micro')\n",
    "    precision_scores.append(precision)\n",
    "    recall = recall_score(labels, preds, average='micro')\n",
    "    recall_scores.append(recall)\n",
    "\n",
    "best_idx = np.argmax(f1_scores)\n",
    "best_thr = thresholds[best_idx]\n",
    "best_f1 = f1_scores[best_idx]\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=thresholds, y=f1_scores, mode='lines+markers', name='F1 Score'))\n",
    "fig.add_trace(go.Scatter(x=thresholds, y=accuracy_scores, mode='lines+markers', name='Accuracy'))\n",
    "fig.add_trace(go.Scatter(x=thresholds, y=precision_scores, mode='lines+markers', name='Precision'))\n",
    "fig.add_trace(go.Scatter(x=thresholds, y=recall_scores, mode='lines+markers', name='Recall'))\n",
    "fig.update_layout(\n",
    "    title='Classification Metrics vs. Threshold',\n",
    "    xaxis_title='Threshold',\n",
    "    yaxis_title='Score',\n",
    "    legend_title='Metrics',\n",
    "    width=800,\n",
    "    height=600\n",
    ")\n",
    "fig.show()\n",
    "print(f\"Best global threshold = {best_thr:.2f} (F1={best_f1:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd6e6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print(\"Positive label ratio:\", labels.mean(axis=0).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09891c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = torch.sigmoid(predictions).cpu().numpy()\n",
    "fig = px.histogram(\n",
    "    probs.flatten(),\n",
    "    nbins=50,\n",
    "    title=\"Distribution of predicted probabilities\",\n",
    "    labels={\"value\": \"Sigmoid output\", \"count\": \"Frequency\"},\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114ce1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = np.linspace(0.05, 0.95, 19)\n",
    "pred_counts = [(probs >= t).sum() for t in thresholds]\n",
    "print(list(zip(thresholds, pred_counts)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
