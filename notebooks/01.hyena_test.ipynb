{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34d174a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tarp.model.backbone.untrained.lstm import LstmEncoder\n",
    "from tarp.model.backbone.untrained.hyena import HyenaEncoder\n",
    "from tarp.model.finetuning.classification import ClassificationModel\n",
    "\n",
    "from tarp.services.datasets.classification.multilabel import MultiLabelClassificationDataset\n",
    "from tarp.services.tokenizers.pretrained.dnabert import Dnabert2Tokenizer\n",
    "from tarp.services.datasource.sequence import TabularSequenceSource, CombinationSource, FastaSliceSource\n",
    "\n",
    "\n",
    "from tarp.services.preprocessing.augmentation import (\n",
    "    CombinationTechnique,\n",
    "    RandomMutation,\n",
    "    InsertionDeletion,\n",
    "    ReverseComplement,\n",
    ")\n",
    "\n",
    "from tarp.services.datasets.metric.triplet import MultiLabelOfflineTripletDataset\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc4bb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_columns = (\n",
    "    pl.read_csv(Path(\"../temp/data/processed/labels.csv\")).to_series().to_list()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed61844b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MultiLabelClassificationDataset(\n",
    "    CombinationSource(\n",
    "        [\n",
    "            TabularSequenceSource(\n",
    "                source=Path(\"../temp/data/processed/card_amr.parquet\")\n",
    "            ),\n",
    "            FastaSliceSource(\n",
    "                directory=Path(\"../temp/data/external/sequences\"),\n",
    "                metadata=Path(\"../temp/data/processed/non_amr_genes_10000.parquet\"),\n",
    "                key_column=\"genomic_nucleotide_accession.version\",\n",
    "                start_column=\"start_position_on_the_genomic_accession\",\n",
    "                end_column=\"end_position_on_the_genomic_accession\",\n",
    "                orientation_column=\"orientation\",\n",
    "            ),\n",
    "        ]\n",
    "    ),\n",
    "    Dnabert2Tokenizer(),\n",
    "    sequence_column=\"sequence\",\n",
    "    label_columns=label_columns,\n",
    "    maximum_sequence_length=512,\n",
    "    augmentation=CombinationTechnique(\n",
    "        [\n",
    "            RandomMutation(),\n",
    "            InsertionDeletion(),\n",
    "            ReverseComplement(0.5),\n",
    "        ]\n",
    "    ),\n",
    ")\n",
    "\n",
    "metric_dataset = MultiLabelOfflineTripletDataset(\n",
    "    base_dataset=dataset, label_cache=Path(\"../temp/data/cache/labels_cache.parquet\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7403fa82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tarp.config import HyenaConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d6fd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = HyenaEncoder(\n",
    "    vocabulary_size=dataset.tokenizer.vocab_size,\n",
    "    embedding_dimension=HyenaConfig.embedding_dimension,\n",
    "    hidden_dimension=HyenaConfig.hidden_dimension,\n",
    "    padding_id=dataset.tokenizer.pad_token_id,\n",
    "    number_of_layers=HyenaConfig.number_of_layers,\n",
    "    dropout=HyenaConfig.dropout,\n",
    ")\n",
    "\n",
    "classification_model = ClassificationModel(\n",
    "    encoder=encoder,\n",
    "    number_of_classes=len(label_columns),\n",
    ")\n",
    "\n",
    "classification_model.load_state_dict(\n",
    "    torch.load(\"../temp/checkpoints/HyenaEncoder_20251027_111621.pt\")\n",
    ")\n",
    "\n",
    "# Get the encoder part of the model\n",
    "encoder: HyenaEncoder = classification_model.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee7144e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the model to the dataset to get the embeddings\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Preallocate numpy array for all embeddings\n",
    "num_samples = len(dataset)\n",
    "batch_size = 32\n",
    "embedding_dim = encoder.encoding_size  # Output dimension of encoder.encode\n",
    "embeddings = np.empty((num_samples, embedding_dim), dtype=np.float32)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder.to(DEVICE)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "start_idx = 0\n",
    "encoder.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(dataloader):\n",
    "        input_ids = batch[\"sequence\"].to(DEVICE)\n",
    "        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "        batch_embeddings = encoder.encode(input_ids, attention_mask)\n",
    "        batch_size_actual = batch_embeddings.shape[0]\n",
    "        embeddings[start_idx:start_idx + batch_size_actual] = batch_embeddings.cpu().numpy()\n",
    "        start_idx += batch_size_actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6870838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shape of the embeddings\n",
    "print(\"Embeddings shape:\", embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537391ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "SEED = 69420\n",
    "indices = list(range(len(dataset)))\n",
    "train_indices, temp_indices = train_test_split(\n",
    "    indices, test_size=0.2, random_state=SEED\n",
    ")\n",
    "valid_indices, test_indices = train_test_split(\n",
    "    temp_indices, test_size=0.5, random_state=SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0f2021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into train and test sets\n",
    "train_embeddings = embeddings[train_indices]\n",
    "test_embeddings = embeddings[test_indices]\n",
    "\n",
    "print(\"Train embeddings shape:\", train_embeddings.shape)\n",
    "print(\"Test embeddings shape:\", test_embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a254a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = np.array([\n",
    "    dataset[i]['labels'].numpy() for i in train_indices\n",
    "])\n",
    "test_labels = np.array([\n",
    "    dataset[i]['labels'].numpy() for i in test_indices\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a473598c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN search\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5, metric='cosine', weights='distance')\n",
    "\n",
    "knn.fit(train_embeddings, train_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09767a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(\n",
    "    train_labels,\n",
    "    knn.predict(train_embeddings),\n",
    "    zero_division=0,\n",
    "    target_names=label_columns\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d3d040",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(\n",
    "    test_labels,\n",
    "    knn.predict(test_embeddings),\n",
    "    zero_division=0,\n",
    "    target_names=label_columns\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bd9e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Every label which is not \"non-AMR\", extract the index in the\n",
    "amr_label_indices = [\n",
    "    idx for idx, label in enumerate(label_columns) if \"non_amr\" not in label\n",
    "]\n",
    "\n",
    "# Consolidate all the AMR class labels into a single \"AMR\" label and then Combine with non-AMR to get binary labels\n",
    "train_amr_binary = (train_labels[:, amr_label_indices].sum(axis=1) > 0).astype(int)\n",
    "test_amr_binary = (test_labels[:, amr_label_indices].sum(axis=1) > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e31dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_knn = KNeighborsClassifier(n_neighbors=5, metric='cosine', weights='distance')\n",
    "\n",
    "binary_knn.fit(train_embeddings, train_amr_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50f92a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    classification_report(\n",
    "        train_amr_binary,\n",
    "        binary_knn.predict(train_embeddings),\n",
    "        zero_division=0,\n",
    "        target_names=[\"non-AMR\", \"AMR\"],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d224475",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    classification_report(\n",
    "        test_amr_binary,\n",
    "        binary_knn.predict(test_embeddings),\n",
    "        zero_division=0,\n",
    "        target_names=[\"AMR\", \"non-AMR\"],\n",
    "        labels=[1, 0],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4c66a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import plotly.express as px\n",
    "\n",
    "# Import UMAP and t-SNE\n",
    "from umap import UMAP\n",
    "\n",
    "# Run UMAP on the embeddings\n",
    "umap = UMAP(n_components=2)\n",
    "train_embeddings_2d = umap.fit_transform(train_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7696bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for i in range(len(train_embeddings)):\n",
    "    active_labels = [label_columns[j] for j in np.where(train_labels[i] > 0)[0]]\n",
    "    for label in active_labels:\n",
    "        rows.append({\n",
    "            \"x\": train_embeddings_2d[i, 0],\n",
    "            \"y\": train_embeddings_2d[i, 1],\n",
    "            \"label\": label,\n",
    "            \"sample_index\": i,  # to identify duplicates\n",
    "        })\n",
    "\n",
    "df_vis = pl.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb456b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(\n",
    "    df_vis,\n",
    "    x=\"x\",\n",
    "    y=\"y\",\n",
    "    color=\"label\",\n",
    "    hover_data=[\"sample_index\"],\n",
    "    title=\"UMAP visualization of Hyena embeddings (multi-label)\",\n",
    "    labels={\"x\": \"UMAP dim 1\", \"y\": \"UMAP dim 2\", \"color\": \"Gene family\"},\n",
    "    opacity=0.7,\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    width=1024,\n",
    "    height=768,\n",
    "    legend_title=\"Label\",\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a05810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try t-SNE as well\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=SEED)\n",
    "\n",
    "train_embeddings_2d = tsne.fit_transform(train_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d9c108",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for i in range(len(train_embeddings)):\n",
    "    active_labels = [label_columns[j] for j in np.where(train_labels[i] > 0)[0]]\n",
    "    for label in active_labels:\n",
    "        rows.append({\n",
    "            \"x\": train_embeddings_2d[i, 0],\n",
    "            \"y\": train_embeddings_2d[i, 1],\n",
    "            \"label\": label,\n",
    "            \"sample_index\": i,  # to identify duplicates\n",
    "        })\n",
    "df_vis = pl.DataFrame(rows)\n",
    "fig = px.scatter(\n",
    "    df_vis,\n",
    "    x=\"x\",\n",
    "    y=\"y\",\n",
    "    color=\"label\",\n",
    "    hover_data=[\"sample_index\"],\n",
    "    title=\"t-SNE visualization of Hyena embeddings (multi-label)\",\n",
    "    labels={\"x\": \"t-SNE dim 1\", \"y\": \"t-SNE dim 2\", \"color\": \"Gene family\"},\n",
    "    opacity=0.7,\n",
    ")\n",
    "fig.update_layout(\n",
    "    width=1024,\n",
    "    height=768,\n",
    "    legend_title=\"Label\",\n",
    ")\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
